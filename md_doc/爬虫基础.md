[TOC]

[教程来源](https://www.bilibili.com/video/BV1Yh411o7Sz?p=14)

## 1.爬虫简介

抓取互联网上的数据，为我所用。

有了大量的数据，就如同有了一个数据银行一样。

下一步做的就是如何将这些爬取的数据，产品化、商业化。

### 1.1.爬虫合法性探究

#### 1.1.1.爬虫究竟是违法还是合法的？

- 在法律中不被禁止
- 具有违法风险
- 区分为善意爬虫和恶意爬虫

#### 1.1.2.爬虫带来的风险

- 爬虫干扰了被访问网站的正常运营
- 爬虫抓取了受到法律保护的特定类型的数据或信息

#### 1.1.3.如何避免进局子喝茶

- 时常优化自己的程序，避免干扰被访问网站的正常运行
- 在使用、传播爬取到的数据时，审查抓取的内容，如果发现了涉及到用户以及商业机密等敏感内容时，需要及时停止爬取或传播。

### 1.2.爬虫初始深入

#### 1.2.1.爬虫在使用场景中的分类

- 通用爬虫
  - 抓取系统的重要组成部分。
  - 抓取的是一整张页面数据。
- 聚焦爬虫
  - 是建立在通用爬虫的基础之上。
  - 抓取的是页面中特定的局部内容。
- 增量式爬虫
  - 检测网站中数据更新的情况。
  - 只会抓取网站中最新更新出来的数据。

#### 1.2.2.爬虫的矛与盾

- 反爬机制
  - 门户网站，可以通过指定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。
- 反反爬策略
  - 爬虫程序，可以通过指定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站的和数据。

#### 1.2.3.robots.txt协议

君子协议，规定了网站中哪些数据可以被爬虫爬取，哪些数据不可以被爬取。

http://www.baidu.com/robots.txt

## 2.http&https协议

详细请点击[http&https协议]()

### 2.1.http协议

- 概念：就是服务器和客户端，进行数据交互的一种形式。
- 常用请求头信息：
  - User-Agent：请求头的身份标识。
  - Connection：请求完毕后，是断开连接还是保持连接。
- 常用响应头信息：
  - Content-Type：服务器响应回客户端的数据类型。

### 2.2.https协议

- 概念：安全的超文本传输协议。
- 加密方式：
  - 对称密钥加密
  - 非对称密钥加密
  - 证书密钥加密

## 3.Requests模块

requests模块：python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。

作用：模拟浏览器发送请求。

如何使用：(requests模块的编码流程)：

- 指定url
- 发起请求
- 获取响应数据
- 持久化存储

环境安装：`pip install requests`

实战：获取sogou首页的数据

```python
import requests

if __name__ == '__main__':
    # 指定url
    url = 'https://wwww.sogou.com/'
    # 发送get请求，获取响应数据
    response = requests.get(url = url)
    # 将响应数据转化为字符串
    response_text = response.text
    # 本地持久化存储
    with open('sogou.html','w',encoding='utf-8') as fw:
        fw.write(response_text)
```

### 3.1.Requests巩固

#### 3.1.1.深入案例介绍

- 爬取搜狗指定词条，对应的搜索结果页面（简易网页采集器）。
- 破解百度翻译。
- 爬取豆瓣电影分类排行榜中的电影详情数据。
- 爬取肯德基餐厅查询中，指定地点的餐厅数。
- 爬取国家药品监督管理总局基于中华人民共和国化妆品生产许可证相关数据。

#### 3.1.2.简易网页采集器



#### 3.1.3.百度翻译



#### 3.1.4.豆瓣电影



#### 3.1.5.作业



### 3.2.综合练习

#### 3.2.1.药监总局01



#### 3.2.2.药监总局02



#### 3.2.3.药监总局03-screenflow



#### 3.2.4.药监总局04-screenflow



### 3.3.总结回顾



## 4.数据解析概述

### 4.1.图片数据爬取



### 4.2.正则解析



### 4.3.bs4解析

#### 4.3.1.bs4解析概述



#### 4.3.2.bs4解析具体使用讲解



#### 4.3.3.bs4解析案例实战

### 4.4.xpath解析

#### 4.4.1.xpath解析基础



#### 4.4.2.xpath实战

##### 4.4.2.1.xpath-58二手房



##### 4.4.2.2.xpath-4k图片解析下载



##### 4.4.2.3.xpath-全国城市名称爬取



##### 4.4.2.4.作业



## 5.验证码识别

### 5.1.验证码识别简介



### 5.2.云打码使用流程



### 5.3.古诗文网验证码识别



## 6.模拟登陆

### 6.1.模拟登陆实现流程梳理



### 6.2.人人网模拟登陆



### 6.3.模拟登陆cookie操作



## 7.代理

### 7.1.代理理论讲解



### 7.2.代理在爬虫中的应用



## 8.异步爬虫

### 8.1.异步爬虫概述



### 8.2.异步爬虫-多进程&多线程



### 8.3.异步爬虫-进程池&线程池



### 8.4.异步爬虫-线程池的基本使用



### 8.5.异步爬虫-线程池案例应用



## 9.协程

### 9.1.协程相关概念

#### 9.1.1.异步编程



#### 9.1.2.协程



#### 9.1.3.协程意义



#### 9.1.4.asyncio异步编程

##### 9.1.4.1asyncio事件循环



##### 9.1.4.2.快速上手



##### 9.1.4.3.await关键词



##### 9.1.4.4.task对象



##### 9.1.4.5.future对象



##### 9.1.4.6.concurrent的future对象



##### 9.1.4.7.异步和非异步混合案例



##### 9.1.4.8.异步迭代器



##### 9.1.4.9.异步上下文管理



##### 9.1.4.10.uvloop



##### 9.1.4.11.案例-异步操作redis



##### 9.1.4.12.案例-异步操作mysql



##### 9.1.4.13.FastApi框架异步



##### 9.1.4.14.异步爬虫



##### 9.1.4.15.总结



### 9.2.协程相关操作



### 9.3.多任务异步协程，实现异步爬虫



## 10.aiohttp模块

### 10.1.aiohttp模块引出



### 10.2.aiohttp+多任务异步协程，实现异步爬虫



## 11.selenium

### 11.1.selenium简介



### 11.2.selenium初试



### 11.3.selenium其他自动化操作



### 11.4.iframe处理+动作链



### 11.5.selenium的模拟登陆



### 11.6.无头浏览器+规避检测



### 11.7.超级鹰的基本使用



## 12.scrapy

### 12.1.scrapy框架初识

- 什么是框架
  - 就是一个集成了很多的功能，并且 有很强通用性的一个项目模板。
- 如何学习框架
  - 专门学习框架封装的各种功能的详细用法
- 什么是scrapy
  - 爬虫中封装好的一个明星框架。
  - 功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式。

### 12.2.scrapy环境安装

- 环境安装

  - linux或mac系统

    - `pip install scrapy`

  - windows系统

    - `pip install scrapy`

      测试：在终端里录入`scrapy`命令，没有报错即表示安装成功。

### 12.3.scrapy基本使用

- scrapy使用流程
  - 创建工程

    - `scrapy startproject ProName`

  - 进入工程目录

    - `cd ProName`

  - 创建爬虫文件

    - `scrapy genspider SpiderName www.xxx.com`

      ```python
      import scrapy
      
      
      class FirstSpider(scrapy.Spider):
          # 爬虫文件的名称，就是爬虫文件的唯一标识
          name = 'first'
          # 允许的域名：用来限定start_urls中，哪些url可以进行请求发送
          allowed_domains = ['www.baidu.com']
          # 起始的url列表：该列表中存放的url会被scrapy自动的进行请求的发送，可以有多个
          start_urls = ['http://www.baidu.com/','http://www.sogou.com']
      	# 作用于数据解析：response参数表示的就是，请求成功后对应的响应对象
          # 会被调用多次，由start_urls中的元素个数决定的
          def parse(self, response):
              pass
      
      ```

    - 设置`ROBOTSTXT=False`

    - 设置`LOG_LEVEL='ERROR'`

  - 编写相关操作代码

  - 执行工程

    - `scrapy crawl SpiderName`

### 12.4.scrapy数据解析

爬取糗事百科[https://www.qiushibaike.com/text/](https://www.qiushibaike.com/text/)

```python
import scrapy


class QiushiSpiderSpider(scrapy.Spider):
    name = 'qiushi_spider'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://www.qiushibaike.com/text/']

    def parse(self, response):
        # 解析作者的名称+段子内容
        div_list = response.xpath('//div[@class="col1 old-style-col1"]/div')
        # print(div_list)
        for div  in div_list:
            # xpath返回的是列表，但是列表元素一定是selector类型的对象
            # extract可以将selector对象中，data参数的存储的字符串提取出来
            # author = div.xpath('./div[1]/a[2]/h2/text()')[0].extract()
            author = div.xpath('./div[1]/a[2]/h2/text()').extract_first()

            # 如果列表调用了extract之后，则表示将列表中的每一个selecor对象中data对应的字符串提取了出来
            content = div.xpath('./a[1]/div/span//text()').extract()
            content = ''.join(content)
            print(author,content)
            break
```

设置`USER_AGENT`

运行`scrapy crawl qiushi_spider`

### 12.5.持久化存储

#### 12.5.1.基于终端指令的持久化存储

只可以将parse方法的返回值存储到本地的文本文件中。

注意：持久化存储对应的文本文件的类型，只可以为：`json`、`jsonlines`、`jl`、`csv`、`xml`、`marshal`、`pickle`

指令：`scrapy crawl qiushi_spider -o ./qiushi.csv`

好处：简洁高效便捷

缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）

#### 12.5.2.基础管道持久化存储

编码流程：

- 数据解析

- 在iem类中定义相关的属性

- 将解析的数据，封装存储到item类型的对象中

  ```python
  # items.py
  import scrapy
  
  
  class QiushiItem(scrapy.Item):
      # define the fields for your item here like:
      # name = scrapy.Field()
      author = scrapy.Field()
      content = scrapy.Field()
      # pass
  
  ```

- 将item类型的数据，提交给管道进行持久化存储的操作

- 在管道类的process_item中，要将其接收到的item对象中存储的数据，进行持久化存储操作

  - process_item

    - 专门用来处理item类型的对象
    - 该方法可以接收爬虫文件提交过来的item对象
    - 该方法没接收一个item，就会被调用一次

    ```python
    # pipelines.py
    from itemadapter import ItemAdapter
    
    
    class QiushiPipeline:
        fp = None
        # 重写父类的方法，该方法只会在开始爬虫的时候，被调用一次
        def open_spider(self, spider):
            print('爬虫开始...')
            self.fp = open('./qiushi.txt','w',encoding='utf-8')
    
        # 专门用来处理item类型对象
        def process_item(self, item, spider):
            author = item['author']
            content = item['content']
    
            self.fp.write(author + ':' + content + '\n')
            return item # 这里如果写了return，则item会传递给下一个即将执行的管道类，默认都是加上
    
        # 结束爬虫时，会被调用一次
        def close_spider(self,spider):
            print('爬虫结束...')
    
    ```

- 在配置文件中开启管道

  ```python
  ITEM_PIPELINES = {
      # 数值表示优先级，数值越小，优先级越高
     'qiushi.pipelines.QiushiPipeline': 300,
  }
  ```

- 备注：如果有匿名用户，则会报错

  - 完善author的xpath：

    ```python
    autho = div.xpath('./div[1]/a[2]/h2/text() | ./div[1]/span/h2/text()').extarct_first()
    ```

- 好处：通用性强

- 面试题：将爬取到的 数据一份存储到本地，一本存储到数据库，如何实现？

  - 在管道文件中定义多个管道类：

    ```python
    # 管道文件中，一个管道类对应将一组数据存储到一个平台或一个载体中
    class mysqlPipeline(object):
        conn = None
        pool = None
        value = ''
        def open_spider(self, spider):
            self.pool = ConnectionPool(host='127.0.0.1',port=6379,password='foobared', decode_responses=True)
    
        def process_item(self, item, spider):
            self.conn = Redis(connection_pool=self.pool)
            self.conn.set('k1','v1')
            value = self.conn.get('k1')
        
        def close_spider(self, spider):
            print(self.value)
    ```

  - 在`ITEM_PIPELINES`中配置：

    ```python
    ITEM_PIPELINES = {
       'qiushi.pipelines.QiushiPipeline': 300,
       'qiushi.pipelines.redisPipeline': 301,
    }
    ```

  - 爬虫文件提交的item，只会给管道文件中第一个被执行的管道类接受

  - process_item中的return item表示将item传递给下一个即将执行的管道类

### 12.6.全站数据爬取

- 基于Spider的全站数据爬取

  - 就是将网站中某板块下的全部页码，对应的页面数据进行爬取

  - 需求：爬取校花网中的照片的名称

  - 实现方式：

    - 将所有页面的url添加到start_urls列表（不推荐）

    - 自行手动进行请求发送

      ```python
      import scrapy
      
      
      class XiaohuaSpider(scrapy.Spider):
          name = 'xiaohua'
          # allowed_domains = ['www.xx.com']
          start_urls = ['http://www.521609.com/tuku/index.html']
      
          # 生成一个通用的url模板（不可变的)
          url = 'http://www.521609.com/tuku/index_%d.html'
          page_num = 2
          def parse(self, response):
              li_list = response.xpath('/html/body/div[4]/div[3]/ul/li')
              for li in li_list:
                  img_name = li.xpath('./a/p/text()').extract_first()
                  print(img_name)
              
              if self.page_num <= 3:
                  new_url = format(self.url % self.page_num)
                  self.page_num += 1
      
                  # 手动发送请求，callback回调函数专门用作数据解析
                  yield scrapy.Request(url = new_url, callback = self.parse)
      ```

### 12.7.五大核心组件

- 引擎（scrapy）
  - 用来处理整个系统的数据流处理，触发事务（核心）
- 调度器（Scheduler）
  - 用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。可以想象成要给URL（抓取网页的网址或者是链接）的优先队列，由它来决定下一个要抓取的网址是什么，同时去除重复的网址。
- 下载器（Downloader）
  - 用于下载网页内容，并将网页内容返回给引擎，下载器是建立在**twisted**这个高效的异步模型上的。
- 爬虫（Spiders）
  - 爬虫是用来干活的，用于从特定网页中提取自己需要的信息，即所谓的实体（item）。用户也可以从中提取出链接，让Scrapy继续抓取下一个页面。
- 项目管理（Pipeline）
  - 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体，验证实体信息有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管理，并经过几个特定的次序处理数据。

#### 12.7.1.请求传参

- 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
- 需求：爬取boss的岗位名称，岗位描述

如果要使用管道进行持久化存储，需要先在item.py中定义item：

```python
class BossproItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # pass
    title = scrapy.Field()
    title_href = scrapy.Field()
```

然后导入item中的类：

```python
from bosspro.items import BossproItem
```

然后实例化item对象，把需要的字段，存到item类型的字段中：

```python
```



#### 12.7.2.scrapy图片爬取



#### 12.7.3.中间件

##### 12.7.3.1.中间件初始

- 下载中间件
  - 位置：引擎和下载器之间
  - 作用：批量拦截到整个工程中所有的请求和响应
  - 拦截请求：
    - UA伪装
    - 代理IP
  - 拦截响应：
    - 篡改响应数据，响应对象

##### 12.7.3.2.中间件-处理请求



#### 12.7.4.案例-网易新闻



#### 12.7.5.crawlspider的全站数据爬取



### 12.8.分布式

#### 12.8.1.分布式概述



#### 12.8.2.分布式搭建



#### 12.8.3.增量式爬虫





